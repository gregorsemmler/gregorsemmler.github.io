<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://gregorsemmler.github.io/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://gregorsemmler.github.io/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="Gregor Semmler" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content=", Entry, " />

<meta property="og:title" content="Variational Inference and Directed Graphical Models "/>
<meta property="og:url" content="https://gregorsemmler.github.io/variational-inference-and-directed-graphical-models.html" />
<meta property="og:description" content="Introduction Previously, we have looked at the general idea of learning a model. One way of learning a model and state representation is through variational inference. In the context of neural networks, the variational auto-encoder [1] is the most common way of doing this. Recall that a variational auto-encoder consists …" />
<meta property="og:site_name" content="Gregor Semmler&#39;s Blog" />
<meta property="og:article:author" content="Gregor Semmler" />
<meta property="og:article:published_time" content="2023-01-15T14:20:00+01:00" />
<meta name="twitter:title" content="Variational Inference and Directed Graphical Models ">
<meta name="twitter:description" content="Introduction Previously, we have looked at the general idea of learning a model. One way of learning a model and state representation is through variational inference. In the context of neural networks, the variational auto-encoder [1] is the most common way of doing this. Recall that a variational auto-encoder consists …">

        <title>Variational Inference and Directed Graphical Models  · Gregor Semmler&#39;s Blog
</title>
        <link href="https://gregorsemmler.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Gregor Semmler&#39;s Blog - Full Atom Feed" />



        <link rel="stylesheet" href="https://files.stork-search.net/basic.css" />
    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://gregorsemmler.github.io/"><span class=site-name>Gregor Semmler's Blog</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://gregorsemmler.github.io
                                    >Home</a>
                                </li>
                                <!-- <li ><a href="https://gregorsemmler.github.io/categories">Categories</a></li> -->
                                <!-- <li ><a href="https://gregorsemmler.github.io/tags">Tags</a></li> -->
                                <li ><a href="https://gregorsemmler.github.io/archives">Archives</a></li>
                                <!-- <li><form class="navbar-search" action="https://gregorsemmler.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li> -->
                                <input class="navbar-search search-query" data-stork="sitesearch" type="text" placeholder="Search"/>
                            </ul>
                        </div>
                    </div>
                </div>
                <div data-stork="sitesearch-output"></div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<main>
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://gregorsemmler.github.io/variational-inference-and-directed-graphical-models.html">
                Variational Inference and Directed Graphical Models
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h2>Introduction</h2>
<p>Previously, we have looked at the general idea of learning a model. One way of learning a model and state representation is through variational inference. In the context of neural networks, the variational auto-encoder [1] is the most common way of doing this. Recall that a variational auto-encoder consists of an encoder and a decoder, similar to a regular auto-encoder. In contrast though, the encoder maps to the parameters of a probability distribution, usually a multivariate normal distribution with diagonal covariance. </p>
<p><img alt="vae_schema" src="https://gregorsemmler.github.io/images/model_learning/vae_schema.png"></p>
<h2>Learning for VAEs</h2>
<p>For our unsupervised learning of the dataset, we want to approximate the true distribution of the dataset <span class="math">\(p^*(o)\)</span> with that of our model <span class="math">\(p_\theta(o)\)</span>.
The Kullback-Leibler (KL) divergence measures how different one probability distribution is from another.
For two discrete probability distributions <span class="math">\(p\)</span> and <span class="math">\(q\)</span> of <span class="math">\(x\)</span> it is defined as 
</p>
<div class="math">$$D_{\mathrm{KL}}(p\mid\mid q)=\sum_{x}p(x)\log\biggl({\frac{p(x)}{q(x)}}\biggr)$$</div>
<p> For two continuous distributions <span class="math">\(p\)</span> and <span class="math">\(q\)</span> of <span class="math">\(x\)</span> it is defined as 
</p>
<div class="math">$$D_{\mathrm{KL}}(p\mid\mid q)=\int_{-\infty}^{\infty}p(x)\log\left(\frac{p(x)}{q(x)}\right)d x$$</div>
<p>We can also write
</p>
<div class="math">$$D_{\mathrm{KL}}(p\mid\mid q)=\mathbb{E}_{p(\mathrm{x})}\big[\log p(\mathrm{x})-\log q(\mathrm{x})\big]$$</div>
<p>So, we want to minimize the Kullback-Leibler (KL) divergence between these two distributions. This measurement is always greater or equal to <span class="math">\(0\)</span> and is <span class="math">\(0\)</span> if the true distributions are equal.</p>
<p>In our case we therefore want to minimize</p>
<div class="math">$$\operatorname*{min}_{\theta}\left\{D_{\mathrm{KL}}(p^{\ast}(o)\parallel\ p_{\theta}(o))\right\}$$</div>
<p>which is equivalent to
</p>
<div class="math">$$\operatorname*{max}_{\theta}\left\{\mathbb{E}_{p^{\ast}(o)}\big[\log p_{\theta}(o)\big]\right\}$$</div>
<p>
By doing a few restatements we get
</p>
<div class="math">$$\log p_{\theta}(\mathrm{o}) = \mathbb{E}_{q_\phi(s|o)}\big[\log p_{\theta}(o) \big]= \mathbb{E}_{q_\phi(s|o)}\big[ \log\left(\frac{p_{\theta}(o,s)}{p_{\theta}(s|o)}\right)\big] = $$</div>
<div class="math">$$=  \mathbb{E}_{q_\phi(s|o)}\big[\log q_\phi(s|o) +  \log p_{\theta}(o,s) - \log p_{\theta}(s|o) - \log q_\phi(s|o) \big] = $$</div>
<div class="math">$$ = \mathbb{E}_{q_\phi(s|o)}\big[\log q_\phi(s|o) - \log p_{\theta}(s|o)\big] + \mathbb{E}_{q_\phi(s|o)}\big[\log p_{\theta}(o,s)  - \log q_\phi(s|o) \big] = $$</div>
<div class="math">$$ = D_{\mathrm{KL}}\big( q_\phi(s|o)\parallel\ p_{\theta}(s|o)\big) + \mathcal{L}(\theta,\phi;o) $$</div>
<p>
Where <span class="math">\(\mathcal{L}(\theta,\phi;o)\)</span> is the so-called evidence lower bound (ELBO) or variational lower bound (VLB) or negative variational free energy for which holds</p>
<div class="math">$$\mathcal{L}(\theta,\phi;o) = \mathbb{E}_{q_\phi(s|o)}\big[\log p_{\theta}(o,s)  - \log q_\phi(s|o) \big]   \leq \log p_{\theta}(\mathrm{o})$$</div>
<p>Because, as mentioned above, for the KL divergence holds</p>
<div class="math">$$D_{\mathrm{KL}}\big( q_\phi(s|o)\parallel\ p_{\theta}(s|o)\big) \geq 0$$</div>
<p>So, we can restate our earlier optimization problem as </p>
<div class="math">$$\operatorname*{max}_{\theta}\left\{\mathcal{L}(\theta,\phi;o)\right\}$$</div>
<p>
We can also rephrase the VLB in the form
</p>
<div class="math">$$\mathbb{E}_{q_\phi(s|o)}\big[\log p_{\theta}(o,s)  - \log q_\phi(s|o) \big] = \mathbb{E}_{q_\phi(s|o)}\big[\log p_{\theta}(o|s) + \log p_{\theta}(s)  - \log q_\phi(s|o) \big] = $$</div>
<div class="math">$$= \mathbb{E}_{q_\phi(s|o)}\big[\log p_{\theta}(o|s)\big] - \mathbb{E}_{q_\phi(s|o)}\big[\log q_\phi(s|o) - \log p_{\theta}(s) \big] = $$</div>
<div class="math">$$= \mathbb{E}_{q_\phi(s|o)}\big[\log p_{\theta}(o|s)\big] - D_{\mathrm{KL}}\big( q_\phi(s|o)\parallel\ p_{\theta}(s)\big) $$</div>
<p>
Here, the first addend can be interpreted as a reconstruction error which captures the average accuracy of the whole pass through the encoder and then the decoder.  The second can be seen as a regularization error that keeps the approximate distribution <span class="math">\(q_\phi(s|o)\)</span> close to the prior distribution over the latent variables <span class="math">\(p_{\theta}(s)\)</span>.</p>
<p>For a dataset of observations <span class="math">\(\mathrm{O}\)</span> of size <span class="math">\(N\)</span> the total VLB is the sum over all the datapoints <span class="math">\(o_n\)</span></p>
<div class="math">$$\mathcal{L}(\theta,\phi;\mathrm{O}) = \sum_{i=1}^{N} \mathbb{E}_{q_\phi(s_i|o_i)}\big[\log p_{\theta}(o_i,s_i)  - \log q_\phi(s_i|o_i) \big] = $$</div>
<div class="math">$$= \sum_{i=1}^{N} \mathbb{E}_{q_\phi(s_i|o_i)}\big[\log p_{\theta}(o_i|s_i)\big] - \sum_{i=1}^{N}D_{\mathrm{KL}}\big( q_\phi(s_i|o_i)\parallel\ p_{\theta}(s_i)\big)$$</div>
<p> 
However, calculating the expectation with respect to <span class="math">\(q_\phi(s_i|o_i)\)</span> is analytically intractable. One approach therefore is to approximate it using stochastic gradient descent using a Monte Carlo estimate of <span class="math">\(L\)</span> samples drawn i.i.d. from <span class="math">\(q_\phi(s_i|o_i)\)</span> 
</p>
<div class="math">$$\mathbb{E}_{q_\phi(s_i|o_i)}\big[\log p_{\theta}(o_i|s_i)\big] \approx \frac{1}{L} \sum_{l=1}^{L}\log p_{\theta}(o_i|s_i)$$</div>
<p>
We arrive at the approximate loss function
</p>
<div class="math">$$\widetilde{\mathcal{L}}(\theta,\phi;\mathrm{O}) = \sum_{i=1}^{N} \frac{1}{L} \sum_{l=1}^{L}\log p_{\theta}(o_i|s_i) - \sum_{i=1}^{N}D_{\mathrm{KL}}\big( q_\phi(s_i|o_i)\parallel\ p_{\theta}(s_i)\big) \approx \mathcal{L}(\theta,\phi;\mathrm{O})$$</div>
<p>We want to extend the standard VAE to a case where we have sequences of random variables. To do this, we first want to introduce directed graphical models.</p>
<h2>Directed Graphical Models</h2>
<p>Let <span class="math">\(X, Y, Z\)</span> be three events. We say that <span class="math">\(X\)</span> and <span class="math">\(Y\)</span> are <em>conditionally independent</em> of each other given <span class="math">\(Z\)</span>, if and only if <span class="math">\(p(Z) &gt; 0\)</span> and
</p>
<div class="math">$$p(X,Y|Z)=p(X|Z)p(Y|Z)$$</div>
<p>
Or equivalently
</p>
<div class="math">$$p(X|Y,Z)=p(X|Z)$$</div>
<p>
We can also write this as
</p>
<div class="math">$$X\perp Y|Z$$</div>
<p>We can analogously define the case in which <span class="math">\(X, Y\)</span> and <span class="math">\(Z\)</span> are random variables.</p>
<p>Now given a number of random variables we can represent the factorization of the joint distribution of all of them graphically with a <em>directed graphical model</em> also known as a <em>bayesian network</em>. Every node in the graph stands for a random variable and an edge for a conditional dependency. </p>
<p>A graph <span class="math">\(G = (\mathcal{V},\mathcal{E})\)</span> consists of a set of vertices <span class="math">\(\mathcal{V}=\{1,\ldots,V\}\)</span> and a set of edges <span class="math">\(\mathcal{E} = \{(s,t)\::\:s,t\:\in\:\mathcal{V}\}\)</span>. </p>
<p>A graph is called <em>undirected</em> if for all <span class="math">\(s,t \in \mathcal{V}\)</span>: 
</p>
<div class="math">$$(s,t) \in \mathcal E \Rightarrow (t,s) \in \mathcal E$$</div>
<p>
In other words, in an undirected graph for all two nodes <span class="math">\(s\)</span> and <span class="math">\(t\)</span>, either there exists an edge both from <span class="math">\(s\)</span> to <span class="math">\(t\)</span> and also from <span class="math">\(t\)</span> to <span class="math">\(s\)</span>, or none. If a graph is not <em>undirected</em>, it is called <em>directed</em>.</p>
<p>In a <em>directed graphical model</em> the joint distribution over the vector <span class="math">\({\mathbf x}_{1:N}\)</span> of random variables can be represented as
</p>
<div class="math">$$p({\mathbf x}_{1:N})=\prod_{i=1}^{N}p(x_i|\mathrm{pa}(x_i))$$</div>
<p>
Where <span class="math">\(\mathrm{pa}(s)\triangleq{\big\{}t: (t,s) \in \mathcal{E}\big\}\)</span> denote the <em>parents</em> of <span class="math">\(s \in \mathcal{V}\)</span>.</p>
<p>In a directed graph we want to determine which nodes are conditionally independent of all other nodes. A set of nodes <span class="math">\(A\)</span> is <strong>d-separated</strong> from another set of nodes <span class="math">\(B\)</span> given a observed set of nodes <span class="math">\(C\)</span> if and only if each undirected path from every node <span class="math">\(a \in A\)</span> to every node <span class="math">\(b \in B\)</span>  is d-separated by <span class="math">\(C\)</span>. An undirected path from <span class="math">\(a\)</span> to <span class="math">\(b\)</span> is d-separated given <span class="math">\(C\)</span> if and only if at least one of the following statements is true [2]:</p>
<ol>
<li>It contains a chain of the form <span class="math">\(a\ \rightarrow\ c\ \rightarrow\ b\)</span> or <span class="math">\(a\ \leftarrow\ c\ \leftarrow\ b\)</span>, for a node <span class="math">\(c \in C\)</span></li>
<li>It contains a fork of the form <span class="math">\(a\ \leftarrow\ c\ \rightarrow\ b\)</span>, for a node <span class="math">\(c \in C\)</span>.</li>
<li>It contains a v-structure of the form <span class="math">\(a\ \leftarrow\ d\ \rightarrow\ b\)</span>, for a node <span class="math">\(d \notin C\)</span> where for all descendants <span class="math">\(e\)</span> of <span class="math">\(d\)</span> we also have <span class="math">\(e \notin C\)</span>. The descendants are all nodes than be reached from the node following directed paths in the graph (children, grand-children and so forth).</li>
</ol>
<p>Consider this example of a simple graphical model with the random variables <span class="math">\(D, E, F, G, H, I\)</span></p>
<p><img alt="simple_graphical_model_example" src="https://gregorsemmler.github.io/images/model_learning/simple_graphical_model_example.png"></p>
<p>According to the earlier formula we can express the joint distribution by following the topological ordering. Simply put, we begin at a node with no parents, then continue with another one without parents if the first node was to be removed. This is repeated until we have covered all nodes.</p>
<div class="math">$$p(D,E,F,G,H,I)=p(D)p(E\vert D)p(G\vert D)p(F\vert E)p(H\vert E, G)p(I\vert E)$$</div>
<p>We can also observe that the node <span class="math">\(E\)</span> d-separates the nodes <span class="math">\(D, G, H\)</span> from the nodes <span class="math">\(F, I\)</span>. This means we can make the following conditional independence statements:</p>
<div class="math">$$p(D,G,H \vert E, F, I) = p(D,G,H \vert E)$$</div>
<div class="math">$$p(F, I \vert D, E, G, H) = p(F, I \vert E)$$</div>
<div class="math">$$p(D, F, G, H, I \vert E) = p(D, G, H\vert E)p(F,I \vert E)$$</div>
<h2>Summary</h2>
<p>To wrap up, we looked at the basic principle of VAEs and the derivation of their loss function, including the ELBO. We also looked at directed graphical models and the concept of conditional independence. <a href="https://gregorsemmler.github.io/state-space-models-and-variational-inference.html">Next up,</a> we will look at state space models and how to extend the concept of a VAEs to sequences of random variables.</p>
<h2>References</h2>
<p>[1] D. P. Kingma and M. Welling. <em>Auto-Encoding Variational Bayes</em>. arXiv preprint arXiv:1312.6114. 2013</p>
<p>[2] K. P. Murphy. <em>Machine learning: a probabilistic perspective</em>. 2012</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


             
 
            
            
            







<section>
    <h6 style="display:none;">Comments</h6>
    <p id="comment-message"> </p>

    <div class="accordion" id="accordion2">
        <div class="accordion-group">
            <div class="accordion-heading">
                <a class="accordion-toggle disqus-comment-count comment-count collapsed"
                   data-toggle="collapse"
                   data-parent="#accordion2"
                   data-disqus-identifier="https://gregorsemmler.github.io/variational-inference-and-directed-graphical-models.html"
                   href="https://gregorsemmler.github.io/variational-inference-and-directed-graphical-models.html#comment_thread"
                   id="comment-accordion-toggle">
                    Comments
                </a>
            </div>
            <div id="comment_thread" class="accordion-body collapse">
                <div class="accordion-inner">
                    <div class="comments">
                        <div id="disqus_thread"></div>
                        <script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = 'https://gregorsemmler.github.io/variational-inference-and-directed-graphical-models.html';
    this.page.identifier = 'https://gregorsemmler.github.io/variational-inference-and-directed-graphical-models.html';
    };
     */
    var disqus_identifier = 'https://gregorsemmler.github.io/variational-inference-and-directed-graphical-models.html';
    var disqus_url = 'https://gregorsemmler.github.io/variational-inference-and-directed-graphical-models.html';
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://gregor-s.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2023-01-15T14:20:00+01:00">So 15 Januar 2023</time>
            <h4>Category</h4>
            <a class="category-link" href="https://gregorsemmler.github.io/categories#entry-ref">Entry</a>
<h4>Contact</h4>
<div id="sidebar-social-link">
</div>
            





            





        </section>
</div>
</article>
</main>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://gregorsemmler.github.io/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script src="https://files.stork-search.net/releases/v1.5.0/stork.js"></script>
        <script>
            stork.register("sitesearch", "https://gregorsemmler.github.io/search-index.st")
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>


    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>