<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://gregorsemmler.github.io/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://gregorsemmler.github.io/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="Gregor Semmler" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content=", Entry, " />

<meta property="og:title" content="State Space Models and Variational Inference "/>
<meta property="og:url" content="https://gregorsemmler.github.io/state-space-models-and-variational-inference.html" />
<meta property="og:description" content="Introduction In the last article we have looked at VAEs, how to train them and the concept of directed graphical models to represent relationships between random variables. Now we will look at state space models, an application of directed graphical models to portray sequences of random variables and how to …" />
<meta property="og:site_name" content="Gregor Semmler&#39;s Blog" />
<meta property="og:article:author" content="Gregor Semmler" />
<meta property="og:article:published_time" content="2023-02-25T15:20:00+01:00" />
<meta name="twitter:title" content="State Space Models and Variational Inference ">
<meta name="twitter:description" content="Introduction In the last article we have looked at VAEs, how to train them and the concept of directed graphical models to represent relationships between random variables. Now we will look at state space models, an application of directed graphical models to portray sequences of random variables and how to …">

        <title>State Space Models and Variational Inference  · Gregor Semmler&#39;s Blog
</title>
        <link href="https://gregorsemmler.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Gregor Semmler&#39;s Blog - Full Atom Feed" />



        <link rel="stylesheet" href="https://files.stork-search.net/basic.css" />
    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://gregorsemmler.github.io/"><span class=site-name>Gregor Semmler's Blog</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://gregorsemmler.github.io
                                    >Home</a>
                                </li>
                                <!-- <li ><a href="https://gregorsemmler.github.io/categories">Categories</a></li> -->
                                <!-- <li ><a href="https://gregorsemmler.github.io/tags">Tags</a></li> -->
                                <li ><a href="https://gregorsemmler.github.io/archives">Archives</a></li>
                                <!-- <li><form class="navbar-search" action="https://gregorsemmler.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li> -->
                                <input class="navbar-search search-query" data-stork="sitesearch" type="text" placeholder="Search"/>
                            </ul>
                        </div>
                    </div>
                </div>
                <div data-stork="sitesearch-output"></div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<main>
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://gregorsemmler.github.io/state-space-models-and-variational-inference.html">
                State Space Models and Variational Inference
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h2>Introduction</h2>
<p>In the last article we have looked at VAEs, how to train them and the concept of directed graphical models to represent relationships between random variables. Now we will look at state space models, an application of directed graphical models to portray sequences of random variables and how to extend the principles of VAEs to them.</p>
<h2>State Space Models</h2>
<p>State space models (SSM) consists of several types of real-valued random variables. Here, we denote the hidden real-valued variables as <span class="math">\(s_t\)</span>, the input variables (action) as <span class="math">\(a_t\)</span>, and the output variables <span class="math">\(o_t\)</span> (which are the reconstructed observations) for each discrete-valued time step <span class="math">\(t\)</span>. 
They can be represented as directed graphical models, where each node represents a random variable, and an edge represents a conditional dependency [1].</p>
<h3>Kalman Filter</h3>
<p>A special case are linear-gaussian SSMs [1], also known as linear dynamical systems. In this case, the transition and observation models are linear, with added Gaussian noise. In particular, the transition model has the form
</p>
<div class="math">$$\mathbf{s}_{t}=\mathbf{A}_{t}\mathbf{s}_{t-1}+\mathbf{B}_{t}\mathbf{a}_{t}+ {\mathcal{N}}(\mathbf{0},\mathbf{Q}_{t})$$</div>
<p>
And the observation model has the form
</p>
<div class="math">$$\mathbf{o}_{t}=\mathbf{C}_{t}\mathbf{s}_{t}+\mathbf{D}_{t}\mathbf{a}_{t}+ {\mathcal{N}}(\mathbf{0},\mathbf{R}_{t})$$</div>
<p>
Where <span class="math">\(\mathbf{A_t, B_t, C_t, D_t, Q_t, R_t}\)</span> are matrices of appropriate size. If they are independent of the time, the model is called <strong>stationary</strong> and we can drop the time index <span class="math">\(t\)</span>.
Often, we model the observations to only be dependent on the latent variables, so in this case the second equation simplifies to
</p>
<div class="math">$$\mathbf{o}_{t}=\mathbf{C}_{t}\mathbf{s}_{t} + {\mathcal{N}}(\mathbf{0},\mathbf{R}_{t})$$</div>
<p>
Combined with a stationary model we get the transition model
</p>
<div class="math">$$\mathbf{s}_t =\mathbf{A} \mathbf{s}_{t-1}+\mathbf{B}\mathbf{a}_{t}+ {\mathcal{N}}(\mathbf{0},\mathbf{Q})$$</div>
<p>
and observation model
</p>
<div class="math">$$\mathbf{o}_{t}=\mathbf{C} \mathbf{s}_{t} + {\mathcal{N}}(\mathbf{0},\mathbf{R})$$</div>
<p>We can represent this as a graphical model:</p>
<p><img alt="ssm_example" src="https://gregorsemmler.github.io/images/model_learning/ssm_example.png"></p>
<p>The <strong>kalman filter</strong> allows to solve this exactly. We can predict the hidden state <span class="math">\(s_t\)</span> based on the history of the previous observations <span class="math">\(\mathbf{o_{1:t-1}}\)</span> and actions <span class="math">\(\mathbf{a_{1:t}}\)</span> [1]. </p>
<div class="math">$$p(\mathbf{s}_{t}|\mathbf{o_{1:t-1}},\mathbf{a_{1:t}}) = {\mathcal{N}}(s_{t}|\mu_{t},\Sigma_{t}) = \int_{-\infty}^{\infty} \mathcal{N}(\mathbf{s}_{t}|\mathbf{A}_{t}\mathbf{s}_{t-1}+\mathbf{B}_{t}\mathbf{a}_{t},\mathbf{Q}_{t})\mathcal{N}(\mathbf{s}_{t-1}|\mu_{t-1},\mathbf{\Sigma}_{t-1})d\mathbf{s}_{t-1}$$</div>
<h2>Variational Autoencoders for State Space Models</h2>
<p>The original VAE does not contain temporal information, so the encoder learns a latent representation where each data point is independent of its time index <span class="math">\(t\)</span>, so no transition information is contained. Several works have tried to extend the unsupervised representation learning to correlated temporal sequences in the years after [2-12].
These approaches all vary in how they define the dependencies between the observed and latent variables and their generative and inference models. Recurrent neural networks are also used as part of the generative and inference models. 
But all models use the basic VAE approach of an inference model and the maximization of a variational lower bound. They all also use continuous latent random variables and use discrete time steps of observed and latent random vectors. </p>
<h3>Learning</h3>
<p>We now consider models which handle the case where we have a sequence of observations <span class="math">\(o_{1:T}\)</span>, of latent variables (or states) <span class="math">\(s_{1:T}\)</span>, and (optionally) of actions <span class="math">\(a_{1:T}\)</span>. In general, we assume that the variables within those sequences are correlated with each other. 
The structure of the dependencies of the variables is usually written as a product of conditional distributions using the chain rule. Here, the different orderings can be chosen which allows for different sampling processes and implementations. 
Often it is assumed that the distribution of a variable at time <span class="math">\(t\)</span> only depends on other variables of time <span class="math">\(t\)</span> or earlier.</p>
<p>We usually assume that the actions are deterministic, so we are interested only interested in the distributions over the observations and latent variables. So, we can model the joint distribution of observations and latent variables, conditioned on the actions according to the chain rule, following the time steps <span class="math">\(t\)</span>, as [13]
</p>
<div class="math">$$p({\bf o}_{1:T},{\bf s}_{1:T}\vert{\bf a}_{1:T})=\prod_{t=1}^{T}p({\bf o}_{t},{\bf s}_{t}\vert{\bf o}_{1:t-1},{\bf s}_{1:t-1},{\bf a}_{1:t}) =$$</div>
<div class="math">$$= \prod_{t=1}^{T}p({\bf o}_{t}\vert{\bf o}_{1:t-1},{\bf s}_{1:t},{\bf a}_{1:t})p({\bf s}_{t}\vert{\bf o}_{1:t-1},{\bf s}_{1:t-1},{\bf a}_{1:t})$$</div>
<p>The implementation is done with a combination of feedforward neural networks and recurrent neural networks (RNNs). The latter are used to capture the aggregated previous data (for example the past states). How this works is that an internal state variable <span class="math">\(h_t\)</span> of the RNN is computed at each time step <span class="math">\(t\)</span>. We can denote such implementations by including <em>deterministic</em> nodes in our graphical model for these state variables.</p>
<p>According to the chain rule, the posterior distribution of the hidden states can be written as
</p>
<div class="math">$$p_{\theta}(\mathbf{s}_{1:T} | \mathbf{o}_{1:T},\mathbf{a}_{1:T})=\prod_{t=1}^{T}p_{\theta}(\mathbf{s}_{t}|\mathbf{s}_{1:t-1},\mathbf{o}_{1:T},\mathbf{a}_{1:T})$$</div>
<p>
Depending on the conditional dependencies of the model, this can be simplified.</p>
<p>The VLB in this case can be derived analogously to the standard VAE case above. Combined with the above factorization, we arrive at </p>
<div class="math">$$\mathcal{L}(\theta,\phi;{\bf o}_{1:T},{\bf a}_{1:T})=\mathbb{E}_{q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T},{\bf a}_{1:T})}\big[\log p_{\theta}({\bf o}_{1:T},{\bf s}_{1:T}|{\bf a}_{1:T}) - \log q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T},{\bf a}_{1:T})\big] =
$$</div>
<div class="math">$$= \sum_{t=1}^{T}{\mathbb{E}_{q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T},{\bf a}_{1:T})}\big[\log p_\theta({\bf o}_{t}\vert{\bf o}_{1:t-1},{\bf s}_{1:t},{\bf a}_{1:t})\big]} -$$</div>
<div class="math">$$ - \sum_{t=1}^{T}{\mathbb{E}_{q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T},{\bf a}_{1:T})}\big[D_{\mathrm{KL}}\big( q_\phi({\bf s}_t|{\bf s}_{1:t-1}, {\bf o}_{1:T},{\bf a}_{1:T})\parallel\ p_{\theta}({\bf s}_t|{\bf o}_{1:t-1},{\bf s}_{1:t-1},{\bf a}_{1:t})\big)\big]}$$</div>
<p>
Again, the first addend can be interpreted as a construction error and the second as a regularization error. In contrast to a VAE, both terms are intractable and need to be estimated, for example with a Monte Carlo sampling approach.</p>
<h2>Summary</h2>
<p>We have now introduced the concept of state space models and derived the general loss function we can use for extended versions of a VAE, designed for state space models and sequences of random variables. In <a href="https://gregorsemmler.github.io/variational-autoencoders-for-state-space-models.html">the next article</a>, we will cover several examples of these in more detail.</p>
<h2>References</h2>
<p>[1] K. P. Murphy. <em>Machine learning: a probabilistic perspective</em>. 2012</p>
<p>[2] J. Bayer and C. Osendorfer. <em>Learning Stochastic Recurrent Networks</em>. ArXiv. 2014</p>
<p>[3] R. G. Krishnan, U. Shalit and D. Sontag. <em>Deep kalman filters</em>. arXiv preprint arXiv:1511.05121. 2015</p>
<p>[4] J. Chung, et al. <em>A recurrent latent variable model for sequential data</em>. Advances in neural information processing systems, 28. 2015</p>
<p>[5] M. Fraccaro, et al. <em>Sequential neural models with stochastic layers</em>. Advances in neural information processing systems, 29. 2016</p>
<p>[6] R. Krishnan, U. Shalit and D. Sontag. <em>Structured Inference Networks for Nonlinear State Space Models</em>. Proceedings of the AAAI Conference on Artificial Intelligence, 31. 2017</p>
<p>[7] M. Fraccaro, et al. <em>A Disentangled Recognition and Nonlinear Dynamics Model for Unsupervised Learning</em>. Advances in neural information processing systems, 30. 3601-3610. 2017</p>
<p>[8] M. Karl, et al. <em>Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data</em>. arXiv preprint arXiv:1605.06432. 2017</p>
<p>[9] A. Goyal, et al. <em>Z-Forcing: Training Stochastic Recurrent Networks</em>. ArXiv. 2017</p>
<p>[10] W. Hsu, Y. Zhang and J. R. Glass. <em>Unsupervised Learning of Disentangled and Interpretable Representations from Sequential Data</em>. 2017</p>
<p>[11] Y. Li and S. Mandt. <em>Disentangled Sequential Autoencoder</em>. 2018</p>
<p>[12] S. Leglaive, et al. <em>A Recurrent Variational Autoencoder for Speech Enhancement</em>. ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP). 371-375. 2020</p>
<p>[13] L. Girin, et al. <em>Dynamical variational autoencoders: A comprehensive review</em>. arXiv preprint arXiv:2008.12595. 2020</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


             
 
            
            
            







<section>
    <h6 style="display:none;">Comments</h6>
    <p id="comment-message"> </p>

    <div class="accordion" id="accordion2">
        <div class="accordion-group">
            <div class="accordion-heading">
                <a class="accordion-toggle disqus-comment-count comment-count collapsed"
                   data-toggle="collapse"
                   data-parent="#accordion2"
                   data-disqus-identifier="https://gregorsemmler.github.io/state-space-models-and-variational-inference.html"
                   href="https://gregorsemmler.github.io/state-space-models-and-variational-inference.html#comment_thread"
                   id="comment-accordion-toggle">
                    Comments
                </a>
            </div>
            <div id="comment_thread" class="accordion-body collapse">
                <div class="accordion-inner">
                    <div class="comments">
                        <div id="disqus_thread"></div>
                        <script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = 'https://gregorsemmler.github.io/state-space-models-and-variational-inference.html';
    this.page.identifier = 'https://gregorsemmler.github.io/state-space-models-and-variational-inference.html';
    };
     */
    var disqus_identifier = 'https://gregorsemmler.github.io/state-space-models-and-variational-inference.html';
    var disqus_url = 'https://gregorsemmler.github.io/state-space-models-and-variational-inference.html';
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://gregor-s.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2023-02-25T15:20:00+01:00">Sa 25 Februar 2023</time>
            <h4>Category</h4>
            <a class="category-link" href="https://gregorsemmler.github.io/categories#entry-ref">Entry</a>
<h4>Contact</h4>
<div id="sidebar-social-link">
</div>
            





            





        </section>
</div>
</article>
</main>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://gregorsemmler.github.io/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script src="https://files.stork-search.net/releases/v1.5.0/stork.js"></script>
        <script>
            stork.register("sitesearch", "https://gregorsemmler.github.io/search-index.st")
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>


    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>