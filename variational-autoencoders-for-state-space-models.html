<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8">
        <meta http-equiv="X-UA-Compatible" content="IE=edge">
        <meta name="viewport" content="width=device-width, initial-scale=1.0">
        <link rel="stylesheet" type="text/css" href="https://gregorsemmler.github.io/theme/css/elegant.prod.9e9d5ce754.css" media="screen">
        <link rel="stylesheet" type="text/css" href="https://gregorsemmler.github.io/theme/css/custom.css" media="screen">

        <link rel="dns-prefetch" href="//fonts.googleapis.com">
        <link rel="preconnect" href="https://fonts.gstatic.com/" crossorigin>

        <meta name="author" content="Gregor Semmler" />

        <meta property="og:type" content="article" />
        <meta name="twitter:card" content="summary">

<meta name="keywords" content=", Entry, " />

<meta property="og:title" content="Variational Autoencoders for State Space Models "/>
<meta property="og:url" content="https://gregorsemmler.github.io/variational-autoencoders-for-state-space-models.html" />
<meta property="og:description" content="Introduction In the last article we have introduced state space models and how to extend the concept of the variational auto-encoder to them and how to train them. We will now look at several examples of VAEs for SSMs in more detail. Deep Kalman Filter The Deep Kalman Filter [1 …" />
<meta property="og:site_name" content="Gregor Semmler&#39;s Blog" />
<meta property="og:article:author" content="Gregor Semmler" />
<meta property="og:article:published_time" content="2023-04-07T08:23:00+02:00" />
<meta name="twitter:title" content="Variational Autoencoders for State Space Models ">
<meta name="twitter:description" content="Introduction In the last article we have introduced state space models and how to extend the concept of the variational auto-encoder to them and how to train them. We will now look at several examples of VAEs for SSMs in more detail. Deep Kalman Filter The Deep Kalman Filter [1 …">

        <title>Variational Autoencoders for State Space Models  · Gregor Semmler&#39;s Blog
</title>
        <link href="https://gregorsemmler.github.io/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Gregor Semmler&#39;s Blog - Full Atom Feed" />



        <link rel="stylesheet" href="https://files.stork-search.net/basic.css" />
    </head>
    <body>
        <div id="content">
            <div class="navbar navbar-static-top">
                <div class="navbar-inner">
                    <div class="container-fluid">
                        <a class="btn btn-navbar" data-toggle="collapse" data-target=".nav-collapse">
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                            <span class="icon-bar"></span>
                        </a>
                        <a class="brand" href="https://gregorsemmler.github.io/"><span class=site-name>Gregor Semmler's Blog</span></a>
                        <div class="nav-collapse collapse">
                            <ul class="nav pull-right top-menu">
                                <li >
                                    <a href=
                                       https://gregorsemmler.github.io
                                    >Home</a>
                                </li>
                                <!-- <li ><a href="https://gregorsemmler.github.io/categories">Categories</a></li> -->
                                <!-- <li ><a href="https://gregorsemmler.github.io/tags">Tags</a></li> -->
                                <li ><a href="https://gregorsemmler.github.io/archives">Archives</a></li>
                                <!-- <li><form class="navbar-search" action="https://gregorsemmler.github.io/search.html" onsubmit="return validateForm(this.elements['q'].value);"> <input type="text" class="search-query" placeholder="Search" name="q" id="tipue_search_input"></form></li> -->
                                <input class="navbar-search search-query" data-stork="sitesearch" type="text" placeholder="Search"/>
                            </ul>
                        </div>
                    </div>
                </div>
                <div data-stork="sitesearch-output"></div>
            </div>
            <div class="container-fluid">
                <div class="row-fluid">
                    <div class="span1"></div>
                    <div class="span10">
<main>
<article itemscope>
<div class="row-fluid">
    <header class="page-header span10 offset2">
        <h1>
            <a href="https://gregorsemmler.github.io/variational-autoencoders-for-state-space-models.html">
                Variational Autoencoders for State Space Models
            </a>
        </h1>
    </header>
</div>

<div class="row-fluid">
        <div class="span8 offset2 article-content">
            
            <h2>Introduction</h2>
<p>In the last article we have introduced state space models and how to extend the concept of the variational auto-encoder to them and how to train them. We will now look at several examples of VAEs for SSMs in more detail.</p>
<h2>Deep Kalman Filter</h2>
<p>The Deep Kalman Filter [1] is based on the same graphical model as the Kalman Filter described above. But instead, neural networks are used to encode the information into a latent space and decode it back into the original dimension.</p>
<p><img alt="ssm_example" src="https://gregorsemmler.github.io/images/model_learning/ssm_example.png"></p>
<p>According to the rules of d-separation we can observe that the <span class="math">\(s_{t-1}\)</span> node separates all observations <span class="math">\(o_{&lt;t}\)</span>, actions <span class="math">\(a_{&lt;t}\)</span>  from <span class="math">\(s_t\)</span>. Or more formally, <span class="math">\(s_t \perp o_i|s_{t-1}\)</span> and <span class="math">\(s_t \perp a_i|s_{t-1}\)</span> for <span class="math">\(i &lt; t\)</span> such that the posterior distribution simplifies to</p>
<div class="math">$$p_{\theta}\left(\mathbf{s}_{1:T}\mid\mathbf{o}_{1:T},\mathbf{a}_{1:T}\right)=\prod_{t=1}^{T}p_{\theta}\left(\mathbf{s}_{t} \mid \mathbf{s}_{t-1},\mathbf{o}_{t:T},\mathbf{a}_{t:T}\right)$$</div>
<p>Similarly, we can deduce the following simplifications:</p>
<div class="math">$$p_\theta\left({\bf o}_{t}\vert{\bf o}_{1:t-1},{\bf s}_{1: t},{\bf a}_{1: t}\right)=p_\theta\left({\bf o}_{t}\vert{\bf s}_{t}\right)$$</div>
<div class="math">$$p_\theta\left(\mathbf{s}_{t}|\mathbf{o}_{1:t-1},\mathbf{s}_{1:t-1},\mathbf{a}_{1:t}\right)=p_\theta \left(\mathbf{s}_{t}|\mathbf{s}_{t-1},\mathbf{a}_{t}\right)$$</div>
<p>
Therefore, the VLB becomes
</p>
<div class="math">$$\mathcal{L}(\theta,\phi;{\bf o}_{1:T},{\bf a}_{1:T}) = \sum_{t=1}^{T}{\mathbb{E}_{q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T},{\bf a}_{1:T})}\big[\log p_\theta({\bf o}_{t}\vert{\bf s}_{t})\big]} -$$</div>
<div class="math">$$- \sum_{t=1}^{T}{\mathbb{E}_{q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T},{\bf a}_{1:T})}\big[D_{\mathrm{KL}}\big( q_\phi(\mathbf{s}_{t}|\mathbf{s}_{t-1},\mathbf{o}_{t:T},\mathbf{a}_{t:T})\parallel\ p_{\theta}(\mathbf{s}_{t}|\mathbf{s}_{t-1},\mathbf{a}_{t})\big)\big]}$$</div>
<p>Krishnan et al. [1] proposed four different neural network models which represent approximations to different probability distributions:
- A neural network for the current state, observation, and action <span class="math">\(q_\phi({\bf s}_t|{\bf o}_{t},{\bf a}_{t})\)</span> 
- A neural network for the recent past and future <span class="math">\(q_\phi({\bf s}_t|{\bf o}_{t-1:t+1}, {\bf a}_{t-1:t+1})\)</span>
- An RNN modelling the complete past <span class="math">\(q_\phi({\bf s}_t|{\bf o}_{1:t},{\bf a}_{1:t})\)</span>
- An RNN modelling the whole sequence <span class="math">\(q_\phi({\bf s}_t|{\bf o}_{1:T},{\bf a}_{1:T})\)</span></p>
<h2>Deep Variational Bayes Filter</h2>
<p>The Deep Variational Bayes Filter [2] modifies the graphical model to include stochastic parameters <span class="math">\(\beta_t\)</span>, which consists of two added terms <span class="math">\(v_t\)</span> and <span class="math">\(w_t\)</span>.  The former is a noise parameter independent of the input, the latter is noise depending on the input. It is a regularizing prior on the </p>
<p>The transition model is assumed to depend on these parameters and factorize in the following way, where <span class="math">\(s_t\)</span> only depends on the previous state <span class="math">\(s_{t-1}\)</span> and no other states as well:</p>
<div class="math">$$p({\bf s}_{1:T}\mid\beta_{1:T},{\bf a}_{1:T})=\prod_{t=1}^{T}p({\bf s}_{t}\mid{\bf s}_{t-1},{\bf a}_{t},\beta_{t})$$</div>
<p>For the observation model the assumption is made that the current state <span class="math">\(s_t\)</span> contains all necessary information about the current observation <span class="math">\(o_t\)</span>:</p>
<div class="math">$$p({\bf o}_{1:T}\mid{\bf s}_{1:T},{\bf a}_{1:T})=\prod_{t=1}^{T}p({\bf o}_{t}\mid{\bf s}_{t})$$</div>
<p>
The approximate recognition model is designed as:
</p>
<div class="math">$$q_{\phi}({\bf{\beta}}_{1:T}\mid{\bf o}_{1:T})=q_{\phi}({\bf w}_{1:T}\mid{\bf o}_{1:T})\,q_{\phi}({\bf v}_{1:T})$$</div>
<p><img alt="dvbf-graphical-model" src="https://gregorsemmler.github.io/images/model_learning/dvbf-graphical-model.png"></p>
<p>The VLB of a DVBF is derived as </p>
<p><span class="math">\({\mathcal{L}}_{\mathrm{DVBF}}(\mathbf{o}_{1:T},\theta,\phi\mid\mathbf{a}_{1:T}) = \mathbb{E}_{q_{\phi}}[\log p_{\theta}(\mathbf{o}_{1:T}\mid\mathbf{s}_{1:T})]-D_{\mathrm{KL}}(q_{\phi}(\beta_{1:T}\mid\mathbf{o}_{1:T},\mathbf{a}_{1:T})\mid p(\beta_{1:T}))\)</span></p>
<p>Where again, the first term is a reconstruction error, and the second term is a regularization term to keep the approximate distribution close to the prior of <span class="math">\(\beta_{1:T}\)</span>. </p>
<p>How the transitions themselves and the parameters <span class="math">\(w_t\)</span> look like, can be chosen according to the application. For example, one approach is to model a locally linear transition model, where the filter learns a number of transition matrices similar to a Kalman filter and weighing parameters <span class="math">\(\alpha_i\)</span> to arrive at the final matrices as a linear combination of these matrices. But nonlinear-transitions can be implemented as well.</p>
<p>An <strong>example implementation</strong> of a DVBF with locally linear transitions can be found here: <a href="https://github.com/gregorsemmler/pytorch-dvbf">https://github.com/gregorsemmler/pytorch-dvbf</a></p>
<h2>Variational Recurrent Neural Networks</h2>
<p>In Variational Recurrent Neural Networks [3] we have internal determinstic states <span class="math">\(h_t\)</span>, denoted by a diamond shape in the graphical model, and implemented with an RNN.</p>
<p><img alt="vrnn-graphical-model-v2-fixed" src="https://gregorsemmler.github.io/images/model_learning/vrnn-graphical-model-v2-fixed.png"></p>
<p>We start with an initial internal state <span class="math">\(h_1\)</span> and action <span class="math">\(a_1\)</span> from which the first state <span class="math">\(s_1\)</span> is generated, after which <span class="math">\(o_1\)</span> is induced from the previous two. Then we get <span class="math">\(h_2\)</span> from <span class="math">\(a_2\)</span>, <span class="math">\(h_1\)</span>, and <span class="math">\(s_1\)</span> and the second state <span class="math">\(s_2\)</span> from <span class="math">\(s_1\)</span> and the process continues in that manner.</p>
<p>The <span class="math">\(h_i\)</span> values can be defined as a deterministic function <span class="math">\(f\)</span> of the other random variables. We can write
</p>
<div class="math">$${\bf h}_t = f\big({\bf o}_{t-1}, {\bf s}_{t-1}, {\bf a}_t, {\bf h}_{t-1}\big)$$</div>
<p>If we unroll this recursively, we get the function <span class="math">\(g\)</span>
</p>
<div class="math">$${\bf h}_t =  f\big({\bf o}_{t-1}, {\bf s}_{t-1}, {\bf a}_t, f\big({\bf o}_{t-2}, {\bf s}_{t-2}, {\bf a}_{t-1}, \ldots f\big({\bf o}_0, {\bf s}_0, {\bf a}_1, {\bf h}_0\big)\big)\big) \triangleq g\big({\bf o}_{1:t-1}, {\bf s}_{1:t-1}, {\bf a}_{1:t}\big)$$</div>
<p>Where <span class="math">\(o_0, s_0, h_0\)</span> are treated as special initialization values and not part of the actual graphical model.</p>
<p>The joint distribution can be represented as
</p>
<div class="math">$$p_\theta\big({\bf o}_{1:T}, {\bf s}_{1:T}, {\bf a}_{1:T}, {\bf h}_{1:T}\big) = \prod_{t=1}^T p_\theta\big({\bf o}_t \mid {\bf s}_t, {\bf h}_t\big) p_\theta\big({\bf s}_t \mid {\bf h}_{t}\big) p_\theta\big({\bf h}_t \mid {\bf o}_{t-1},  {\bf s}_{t-1}, {\bf a}_{t}, {\bf h}_{t-1} \big) p_\theta\big({\bf a}_t\big)$$</div>
<p>
With
</p>
<div class="math">$$p_\theta\big({\bf h}_t \mid {\bf o}_{t-1},  {\bf s}_{t-1}, {\bf a}_{t}, {\bf h}_{t-1} \big) = \delta \big({\bf h}_t; f\big({\bf o}_{t-1}, {\bf s}_{t-1}, {\bf a}_t, {\bf h}_{t-1}\big) \big)$$</div>
<p>
where <span class="math">\(\delta \big(h_t; x \big)\)</span> is the dirac delta which is <span class="math">\(1\)</span> at position <span class="math">\(x\)</span>, otherwise <span class="math">\(0\)</span>.</p>
<p>To retrieve the joint distribution without the deterministic nodes, we can marginalize them out
</p>
<div class="math">$$p_\theta\big({\bf o}_{1:T}, {\bf s}_{1:T}, {\bf a}_{1:T}\big) = \int_{-\infty}^{\infty}\ldots\int_{-\infty}^{\infty}p_\theta\big({\bf o}_{1:T}, {\bf s}_{1:T}, {\bf a}_{1:T}, {\bf h}_{1:T}\big) d{\bf h}_1\ldots d{\bf h}_T =$$</div>
<div class="math">$$= \int \ldots\int  \prod_{t=1}^T p_\theta\big({\bf o}_t \mid {\bf s}_t, {\bf h}_t\big) p_\theta\big({\bf s}_t \mid {\bf h}_{t}\big) \delta \big({\bf h}_t; f\big({\bf o}_{t-1}, {\bf s}_{t-1}, {\bf a}_t, {\bf h}_{t-1}\big) \big) p_\theta\big({\bf a}_t\big) d{\bf h}_1\ldots d{\bf h}_T$$</div>
<div class="math">$$= \prod_{t=1}^T p_\theta\big({\bf o}_t \mid {\bf s}_t, g\big({\bf o}_{1:t-1}, {\bf s}_{1:t-1}, {\bf a}_{1:t}\big)\big) p_\theta\big({\bf s}_t \mid g\big({\bf o}_{1:t-1}, {\bf s}_{1:t-1}, {\bf a}_{1:t}\big)\big) p_\theta\big({\bf a}_t\big) =$$</div>
<p>As <span class="math">\(g\)</span> is deterministic, this is equivalent to
</p>
<div class="math">$$= \prod_{t=1}^T p_\theta\big({\bf o}_t \mid {\bf o}_{1:t-1}, {\bf s}_{1:t-1}, {\bf a}_{1:t}\big) p_\theta\big({\bf s}_t \mid {\bf s}_{1:t-1}, {\bf o}_{1:t-1},  {\bf a}_{1:t}\big) p_\theta\big({\bf a}_t\big)$$</div>
<p>
This we can draw an equivalent graphical model without the deterministic nodes <span class="math">\(h_t\)</span></p>
<p><img alt="vrnn-graphical-model-v2-no-h" src="https://gregorsemmler.github.io/images/model_learning/vrnn-graphical-model-v2-no-h.png"></p>
<p>Based on this simplified graph, we can retrieve the inference model</p>
<div class="math">$$p_{\theta}\left({\bf s}_{1:T}\mid\mathbf{o}_{1:T},\mathbf{a}_{1:T}\right) = \prod_{t=1}^T p_\theta\big({\bf s}_t \mid {\bf s}_{1:t-1}, {\bf o}_{1:T},  {\bf a}_{1:t}\big) $$</div>
<p>
No further simplifications can be made according to d-separation. </p>
<p>This can be approximated with 
</p>
<div class="math">$$q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T})=\prod_{t=1}^{T}q_{\phi}\bigl({\bf s}_{t}|{\bf o}_{t},{\bf h}_{t}\bigr) = \prod_{t=1}^{T}q_{\phi}\bigl({\bf s}_{t}|{\bf o}_{t},g\big({\bf o}_{1:t-1}, {\bf s}_{1:t-1}, {\bf a}_{1:t}\big)\bigr) =$$</div>
<div class="math">$$= \prod_{t=1}^{T}q_{\phi}\bigl({\bf s}_{t}|{\bf s}_{1:t-1}, {\bf o}_{1:t},  {\bf a}_{1:t}\bigr)$$</div>
<p>
For the training, we can initially observe in the graphical model(s) that there are no conditional independence assumptions made. 
So we would use the general VLB for VAEs for sequences as defined in a preceding section:</p>
<div class="math">$$\mathcal{L}(\theta,\phi;{\bf o}_{1:T},{\bf a}_{1:T})= \sum_{t=1}^{T}{\mathbb{E}_{q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T},{\bf a}_{1:T})}\big[\log p_\theta({\bf o}_{t}\vert{\bf o}_{1:t-1},{\bf s}_{1:t},{\bf a}_{1:t})\big]} -$$</div>
<div class="math">$$ - \sum_{t=1}^{T}{\mathbb{E}_{q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T},{\bf a}_{1:T})}\big[D_{\mathrm{KL}}\big( q_\phi({\bf s}_t|{\bf s}_{1:t-1}, {\bf o}_{1:T},{\bf a}_{1:T})\parallel\ p_{\theta}({\bf s}_t|{\bf o}_{1:t-1},{\bf s}_{1:t-1},{\bf a}_{1:t})\big)\big]}$$</div>
<p>Using the previous approximation for <span class="math">\(q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T})\)</span> this becomes</p>
<div class="math">$$\mathcal{L}(\theta,\phi;{\bf o}_{1:T},{\bf a}_{1:T})= \sum_{t=1}^{T}{\mathbb{E}_{q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T},{\bf a}_{1:T})}\big[\log p_\theta({\bf o}_{t}\vert{\bf o}_{1:t-1},{\bf s}_{1:t},{\bf a}_{1:t})\big]} -$$</div>
<div class="math">$$ - \sum_{t=1}^{T}{\mathbb{E}_{q_{\phi}({\bf s}_{1:T}|{\bf o}_{1:T},{\bf a}_{1:T})}\big[D_{\mathrm{KL}}\big( q_\phi({\bf s}_t|{\bf s}_{1:t-1}, {\bf o}_{1:t},{\bf a}_{1:t})\parallel\ p_{\theta}({\bf s}_t|{\bf o}_{1:t-1},{\bf s}_{1:t-1},{\bf a}_{1:t})\big)\big]}$$</div>
<h2>Summary</h2>
<p>In this article, we have looked at several variational auto-encoders for state space models in detail. All of them try to learn internal state representations of observations from an environment. This concludes this first series on model learning and state representation learning for model-based reinforcement learning. </p>
<h2>References</h2>
<p>[1] R. G. Krishnan, U. Shalit and D. Sontag. <em>Deep kalman filters</em>. arXiv preprint arXiv:1511.05121. 2015</p>
<p>[2] M. Karl, et al. <em>Deep Variational Bayes Filters: Unsupervised Learning of State Space Models from Raw Data</em>. arXiv preprint arXiv:1605.06432. 2017</p>
<p>[3] J. Chung, et al. <em>A recurrent latent variable model for sequential data</em>. Advances in neural information processing systems, 28. 2015</p>
<script type="text/javascript">if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) {
    var align = "center",
        indent = "0em",
        linebreak = "false";

    if (false) {
        align = (screen.width < 768) ? "left" : align;
        indent = (screen.width < 768) ? "0em" : indent;
        linebreak = (screen.width < 768) ? 'true' : linebreak;
    }

    var mathjaxscript = document.createElement('script');
    mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#';
    mathjaxscript.type = 'text/javascript';
    mathjaxscript.src = 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.3/latest.js?config=TeX-AMS-MML_HTMLorMML';

    var configscript = document.createElement('script');
    configscript.type = 'text/x-mathjax-config';
    configscript[(window.opera ? "innerHTML" : "text")] =
        "MathJax.Hub.Config({" +
        "    config: ['MMLorHTML.js']," +
        "    TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'none' } }," +
        "    jax: ['input/TeX','input/MathML','output/HTML-CSS']," +
        "    extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js']," +
        "    displayAlign: '"+ align +"'," +
        "    displayIndent: '"+ indent +"'," +
        "    showMathMenu: true," +
        "    messageStyle: 'normal'," +
        "    tex2jax: { " +
        "        inlineMath: [ ['\\\\(','\\\\)'] ], " +
        "        displayMath: [ ['$$','$$'] ]," +
        "        processEscapes: true," +
        "        preview: 'TeX'," +
        "    }, " +
        "    'HTML-CSS': { " +
        "        availableFonts: ['STIX', 'TeX']," +
        "        preferredFont: 'STIX'," +
        "        styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'inherit ! important'} }," +
        "        linebreaks: { automatic: "+ linebreak +", width: '90% container' }," +
        "    }, " +
        "}); " +
        "if ('default' !== 'default') {" +
            "MathJax.Hub.Register.StartupHook('HTML-CSS Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax['HTML-CSS'].FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
            "MathJax.Hub.Register.StartupHook('SVG Jax Ready',function () {" +
                "var VARIANT = MathJax.OutputJax.SVG.FONTDATA.VARIANT;" +
                "VARIANT['normal'].fonts.unshift('MathJax_default');" +
                "VARIANT['bold'].fonts.unshift('MathJax_default-bold');" +
                "VARIANT['italic'].fonts.unshift('MathJax_default-italic');" +
                "VARIANT['-tex-mathit'].fonts.unshift('MathJax_default-italic');" +
            "});" +
        "}";

    (document.body || document.getElementsByTagName('head')[0]).appendChild(configscript);
    (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript);
}
</script>


             
 
            
            
            







<section>
    <h6 style="display:none;">Comments</h6>
    <p id="comment-message"> </p>

    <div class="accordion" id="accordion2">
        <div class="accordion-group">
            <div class="accordion-heading">
                <a class="accordion-toggle disqus-comment-count comment-count collapsed"
                   data-toggle="collapse"
                   data-parent="#accordion2"
                   data-disqus-identifier="https://gregorsemmler.github.io/variational-autoencoders-for-state-space-models.html"
                   href="https://gregorsemmler.github.io/variational-autoencoders-for-state-space-models.html#comment_thread"
                   id="comment-accordion-toggle">
                    Comments
                </a>
            </div>
            <div id="comment_thread" class="accordion-body collapse">
                <div class="accordion-inner">
                    <div class="comments">
                        <div id="disqus_thread"></div>
                        <script>
    /**
     *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
     *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = 'https://gregorsemmler.github.io/variational-autoencoders-for-state-space-models.html';
    this.page.identifier = 'https://gregorsemmler.github.io/variational-autoencoders-for-state-space-models.html';
    };
     */
    var disqus_identifier = 'https://gregorsemmler.github.io/variational-autoencoders-for-state-space-models.html';
    var disqus_url = 'https://gregorsemmler.github.io/variational-autoencoders-for-state-space-models.html';
    (function() { // DON'T EDIT BELOW THIS LINE
        var d = document, s = d.createElement('script');
        s.src = 'https://gregor-s.disqus.com/embed.js';
        s.setAttribute('data-timestamp', +new Date());
        (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>




                    </div>
                </div>
            </div>
        </div>
    </div>
</section>

            <hr/>
        </div>
        <section id="article-sidebar" class="span2">
            <h4>Published</h4>
            <time itemprop="dateCreated" datetime="2023-04-07T08:23:00+02:00">Fr 07 April 2023</time>
            <h4>Category</h4>
            <a class="category-link" href="https://gregorsemmler.github.io/categories#entry-ref">Entry</a>
<h4>Contact</h4>
<div id="sidebar-social-link">
</div>
            





            





        </section>
</div>
</article>
</main>
<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe.
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides.
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo https://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div>
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>                    </div>
                    <div class="span1"></div>
                </div>
            </div>
        </div>
<footer>




    <div id="fpowered">
        Powered by: <a href="http://getpelican.com/" title="Pelican Home Page" target="_blank" rel="nofollow noopener noreferrer">Pelican</a>
        Theme: <a href="https://elegant.oncrashreboot.com/" title="Theme Elegant Home Page" target="_blank" rel="nofollow noopener noreferrer">Elegant</a>
    </div>
</footer>            <script src="//code.jquery.com/jquery.min.js"></script>
        <script src="//netdna.bootstrapcdn.com/twitter-bootstrap/2.3.2/js/bootstrap.min.js"></script>
        <script src="https://gregorsemmler.github.io/theme/js/elegant.prod.9e9d5ce754.js"></script>
        <script>
            function validateForm(query)
            {
                return (query.length > 0);
            }
        </script>
        <script src="https://files.stork-search.net/releases/v1.5.0/stork.js"></script>
        <script>
            stork.register("sitesearch", "https://gregorsemmler.github.io/search-index.st")
        </script>

    <script>
    (function () {
        if (window.location.hash.match(/^#comment-\d+$/)) {
            $('#comment_thread').collapse('show');
        }
    })();
    window.onhashchange=function(){
        if (window.location.hash.match(/^#comment-\d+$/))
            window.location.reload(true);
    }
    $('#comment_thread').on('shown', function () {
        var link = document.getElementById('comment-accordion-toggle');
        var old_innerHTML = link.innerHTML;
        $(link).fadeOut(200, function() {
            $(this).text('Click here to hide comments').fadeIn(200);
        });
        $('#comment_thread').on('hidden', function () {
            $(link).fadeOut(200, function() {
                $(this).text(old_innerHTML).fadeIn(200);
            });
        })
    })
</script>


    </body>
    <!-- Theme: Elegant built for Pelican
        License : MIT -->
</html>